from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from fake_useragent import UserAgent
import json
import time
from datetime import datetime
import random


def get_random_user_agent():
    ua = UserAgent()
    return ua.random


def get_google_scholar_urls(query, file_path, num_pages=10,
                            minscroll=2, maxscroll=5,
                            minsleep=5, maxsleep=10):
    print(f'\033[1m{"query:":<20}\033[0m {query}')
    print(f'\033[1m{"setting things up:":<20}\033[0m {datetime.now().strftime("%H:%M:%S"):>15}')
    urls = []

    chrome_options = Options()
    chrome_options.add_argument('--headless')  # Run Chrome in headless mode (without GUI)

    random_user_agent = get_random_user_agent()
    print(f'\033[1m{"random user agent:":<20}\033[0m {random_user_agent}')
    print(f'\033[1m{". . ."}')
    chrome_options.add_argument(f'user-agent={random_user_agent}')  # Set a random user-agent

    # Specify the path to your ChromeDriver executable
    chrome_path = r"C:\Users\HP\Desktop\chromedriver-win64\chromedriver-win64\chromedriver.exe"

    service = ChromeService(chrome_path)
    driver = webdriver.Chrome(service=service, options=chrome_options)

    print(f'\033[1m{"starting scraping:":<20}\033[0m {datetime.now().strftime("%H:%M:%S"):>15}')
    count = 0
    for page in range(num_pages):
        start = count * 10
        url = f"https://scholar.google.com/scholar?start={start}&q=pain+management&hl=lt&as_sdt=0,5"

        print(f'\033[1m\033[92m{"getting page   "}\033[0m {count + 1}{":"}{datetime.now().strftime("%H:%M:%S"):>18}')
        print(f'  {url}')
        driver.get(url)


        # Wait for 2-5 seconds before interacting with the page
        time.sleep(random.uniform(minscroll, maxscroll))

        # Scroll down to load more results (simulate user scrolling)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

        # Wait for 2-5 seconds after scrolling
        time.sleep(random.uniform(minscroll, maxscroll))

        # Printing page text content for debugging
        print('\033[1m\033[91m' + '-' * 100 + '\033[0m')
        body_text = driver.find_element(By.TAG_NAME, 'body').text
        print(body_text)
        print('\033[1m\033[91m' + '-' * 100 + '\033[0m')

        # Extract URLs from the loaded page
        results = driver.find_elements(By.XPATH, "//h3[@class='gs_rt']//a")
        for result in results:
            urls.append(result.get_attribute('href'))

        # Save URLs after processing each page
        save_urls_to_json(urls, page + 1, file_path)  # Page numbers start from 1

        # Wait for 5-10 seconds before moving to the next page
        sleeptime = random.uniform(minsleep, maxsleep)
        print(f'   sleeping for {sleeptime:.2f} seconds')
        time.sleep(sleeptime)
        count += 1

    driver.quit()  # Close the browser session

    return urls


def save_urls_to_json(urls, page_number, file_path):
    with open(file_path, 'a') as json_file:
        json.dump({'page': page_number, 'urls': urls}, json_file)
        json_file.write('\n')  # Add a newline after each set of URLs for different pages


# Example usage
num_pages = 3

# Execute
start_time = time.time()

query = input('Enter query: ')
maxsleep = int(input('Enter max sleep: '))

minsleep = int(maxsleep/2)
maxscroll = int(minsleep)
minscroll = int(maxscroll/2)

print(f'maxsleep: {maxsleep}, minsleep: {minsleep}, maxscroll: {maxscroll}, minscroll: {minscroll}')

file_path = fr'C:\Users\HP\Desktop\CARD\google_scholar\{query}.json'
result_urls = get_google_scholar_urls(query, file_path, num_pages,
                                      minscroll=minscroll, maxscroll=maxscroll,
                                      minsleep=minsleep, maxsleep=maxsleep)

print('')
print('----------------------------------------')
num_of_results = len(result_urls)
time_passed = time.time() - start_time
print(f"Saved {num_of_results} Google Scholar URLs.")
print(f'\033[1m{"completed in: ":<15}\033[0m {time_passed:>15.2f} seconds')
print(f'\033[1m{"average: ":<15}\033[0m {time_passed/num_pages:>15.2f} seconds / page')
print(f'{datetime.now().strftime("%H:%M:%S"):>15}')
