import json
import time
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.keys import Keys

def get_google_scholar_urls(query, num_pages=10):
    urls = []

    chrome_options = Options()
    chrome_options.add_argument('--headless')  # Run Chrome in headless mode (without GUI)

    # Specify the path to your ChromeDriver executable
    chrome_path = r"C:\Users\HP\Desktop\chromedriver-win64\chromedriver-win64\chromedriver.exe"

    service = ChromeService(chrome_path)
    driver = webdriver.Chrome(service=service, options=chrome_options)

    for page in range(num_pages):
        start = page * 10
        url = f"https://scholar.google.com/scholar?q={query.replace(' ', '+')}&start={start}"

        print(driver.get(url))
        time.sleep(5)  # Wait for 5 seconds for the page to load

        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')
        results = soup.find_all('h3', class_='gs_rt')

        for result in results:
            link = result.find('a')
            if link:
                urls.append(link['href'])

        # Save URLs after processing each page
        save_urls_to_json(urls, page + 1)  # Page numbers start from 1

    driver.quit()  # Close the browser session

    return urls

# Rest of the code remains unchanged



def save_urls_to_json(urls, page_number):
    file_path = r'C:\Users\HP\Desktop\CARD\google_scholar\urls.json'
    with open(file_path, 'a') as json_file:
        json.dump({'page': page_number, 'urls': urls}, json_file)
        json_file.write('\n')  # Add a newline after each set of URLs for different pages


# Example usage
query = "pain management"
num_pages = 5
result_urls = get_google_scholar_urls(query, num_pages)

print(f"Saved {len(result_urls)} Google Scholar URLs.")
