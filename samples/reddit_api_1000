import praw
import passwords
import json

# Replace these with your own API keys
client_id = passwords.client_id
client_secret = passwords.client_secret
user_agent = passwords.user_agent
username = passwords.username
password = passwords.password

# Initialize the Reddit API client
reddit = praw.Reddit(
    client_id=client_id,
    client_secret=client_secret,
    user_agent=user_agent,
    username=username,
    password=password
)

# Specify the subreddit you want to scrape
subreddit_name = "AskDocs"

# Limit the number of posts to retrieve (up to 1000)
post_limit = 1000

# Create a list to store the results
results = []

# Retrieve the latest posts from the subreddit
subreddit = reddit.subreddit(subreddit_name)
for submission in subreddit.new(limit=post_limit):
    # Extract the title and URL of each post
    title = submission.title
    url = submission.url

    # Append the title and URL to the results list as a sublist
    results.append([title, url])


# Print the first 10 results as an example
for i, (title, url) in enumerate(results):
    print(f"{i + 1}. Title: {title}")
    print(f"   Link: {url}\n")

# Define the path for the JSON file
json_file_path = r"C:\Users\HP\Desktop\CARD\reddit posts\posts.json"

# Save the results list to a JSON file
with open(json_file_path, "w") as json_file:
    json.dump(results, json_file)

# Print a message to confirm the save
print(f"Posts saved to {json_file_path}")
