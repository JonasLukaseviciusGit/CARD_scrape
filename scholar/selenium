from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.keys import Keys
from fake_useragent import UserAgent
import json
import time
from datetime import datetime
import random


def get_random_user_agent():
    ua = UserAgent()
    return ua.random


def get_google_scholar_urls(query, num_pages=10):
    print(f'\033[1m{"setting things up:":<20}\033[0m {datetime.now().strftime("%H:%M:%S"):>15}')
    urls = []

    chrome_options = Options()
    chrome_options.add_argument('--headless')  # Run Chrome in headless mode (without GUI)

    random_user_agent = get_random_user_agent()
    print(f'\033[1m{"random user agent:":<20}\033[0m {random_user_agent}')
    print(f'\033[1m{". . ."}')
    chrome_options.add_argument(f'user-agent={random_user_agent}')  # Set a random user-agent

    # Specify the path to your ChromeDriver executable
    chrome_path = r"C:\Users\HP\Desktop\chromedriver-win64\chromedriver-win64\chromedriver.exe"

    service = ChromeService(chrome_path)
    driver = webdriver.Chrome(service=service, options=chrome_options)

    print(f'\033[1m{"starting scraping:":<20}\033[0m {datetime.now().strftime("%H:%M:%S"):>15}')
    count = 0
    for page in range(num_pages):
        start = count * 10
        url = f"https://scholar.google.com/scholar?q={query.replace(' ', '+')}&start={start}"

        print(f'\033[1m{"getting page   "}\033[0m {count + 1}{":"}{datetime.now().strftime("%H:%M:%S"):>18}')
        time.sleep(10)
        driver.get(url)

        # Wait for 2-5 seconds before interacting with the page
        time.sleep(random.uniform(2, 5))

        # Scroll down to load more results (simulate user scrolling)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

        # Wait for 2-5 seconds after scrolling
        time.sleep(random.uniform(2, 5))

        # Extract URLs from the loaded page
        results = driver.find_elements(By.XPATH, "//h3[@class='gs_rt']//a")
        for result in results:
            urls.append(result.get_attribute('href'))

        # Save URLs after processing each page
        save_urls_to_json(urls, page + 1)  # Page numbers start from 1

        # Wait for 5-10 seconds before moving to the next page
        time.sleep(random.uniform(5, 10))
        count += 1

    driver.quit()  # Close the browser session

    return urls


def save_urls_to_json(urls, page_number):
    file_path = r'C:\Users\HP\Desktop\CARD\google_scholar\urls.json'
    with open(file_path, 'a') as json_file:
        json.dump({'page': page_number, 'urls': urls}, json_file)
        json_file.write('\n')  # Add a newline after each set of URLs for different pages


# Example usage
query = "pain management"
num_pages = 3

# Execute
result_urls = get_google_scholar_urls(query, num_pages)

print(f"Saved {len(result_urls)} Google Scholar URLs.")
