from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from fake_useragent import UserAgent
import json
import time
from datetime import datetime
import random
from pymongo import MongoClient

client = MongoClient('localhost', 27017)
db = client['google_scholar']
collection = db['articles']


def get_random_user_agent():
    ua = UserAgent()
    return ua.random


def get_google_scholar_urls(query, y_start, y_end, num_pages=1,
                            minscroll=2, maxscroll=5,
                            minsleep=5, maxsleep=10):
    query = query.replace(' ', '+')
    print(f'\033[1m{"query:":<20}\033[0m {query}')
    print(f'\033[1m{"setting things up:":<20}\033[0m {datetime.now().strftime("%H:%M:%S"):>15}')
    urls = []

    chrome_options = Options()
    chrome_options.add_argument('--headless')  # Run Chrome in headless mode (without GUI)

    random_user_agent = get_random_user_agent()
    print(f'\033[1m{"random user agent:":<20}\033[0m {random_user_agent}')
    print(f'\033[1m{". . ."}')
    chrome_options.add_argument(f'user-agent={random_user_agent}')  # Set a random user-agent

    chrome_path = r"C:\Users\HP\Desktop\chromedriver\chromedriver-win64\chromedriver.exe"

    service = ChromeService(chrome_path)
    driver = webdriver.Chrome(service=service, options=chrome_options)

    print(f'\033[1m{"starting scraping:":<20}\033[0m {datetime.now().strftime("%H:%M:%S"):>15}')
    count = 0
    num_of_results = 0
    print(num_pages)
    for page in range(num_pages):
        urls = []  # Reset the list for each page
        titles = []
        authors = []
        pdfs = []

        url = f"https://scholar.google.com/scholar?start={count}&q={query}&hl=en&as_ylo={y_start}&as_yhi={y_end}"
        print(f'\033[1m\033[92m{"getting page   "}\033[0m {page+1}{":"}{datetime.now().strftime("%H:%M:%S"):>18}')
        print(f'  {url}')
        driver.get(url)

        time.sleep(random.uniform(minscroll, maxscroll))
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(random.uniform(minscroll, maxscroll))
        print('\033[1m\033[91m' + '-' * 100 + '\033[0m')

        # ---------------------------------------- #
        # ---------------------------------------- #
        # ---------------------------------------- #
        html_elements = driver.find_elements(By.XPATH, "//div[contains(@class, 'gs_r gs_or gs_scl')]")
        if len(html_elements) == 0:
            print('End of search')
            break

        for element in html_elements:

            title_element = element.find_element(By.XPATH, ".//h3[@class='gs_rt']")
            if title_element:
                title_element = title_element.text
                print(title_element)
            else:
                print(None)

            author_element = element.find_element(By.XPATH, ".//div[@class='gs_a']")
            print(author_element.text)

            try:
                link_element = element.find_element(By.XPATH, ".//div[@class='gs_or_ggsm']//a")
                link_text = link_element.get_attribute('href')
                _dictionary = {'title': title_element, 'author': author_element.text, 'link': link_text}
                collection.insert_one(_dictionary)
                num_of_results += 1
            except:
                print(None)

            print('-' * 100)
        # ---------------------------------------- #
        # ---------------------------------------- #
        # ---------------------------------------- #

        # Save URLs after processing each page
        total = []
        index = 0
        for url in urls:
            total.append({'url': url, 'title': titles[index], 'author': authors[index]})
            index += 1

        save_urls_to_json(total, page + 1, file_path)  # Page numbers start from 1

        sleeptime = random.uniform(minsleep, maxsleep)
        print(f'   sleeping for {sleeptime:.2f} seconds')
        time.sleep(sleeptime)
        count += 10

    driver.quit()
    return num_of_results


def save_urls_to_json(urls, page_number, file_path):
    with open(file_path, 'a') as json_file:
        json.dump({'page': page_number, 'urls': urls}, json_file)
        json_file.write('\n')



# Execute
start_time = time.time()

query = input('Enter query: ')

num_pages = int(input('Enter number of pages (0 if all): '))
if num_pages == 0:
    num_pages = 1000

maxsleep = int(input('Enter max sleep: '))

y_start = int(input('Enter start year (0 if no): '))
y_end = int(input('Enter end year (0 if no): '))
if y_end == 0:
    y_end = 10000

minsleep = int(maxsleep/2)
maxscroll = int(minsleep)
minscroll = int(maxscroll/2)

print(f'maxsleep: {maxsleep}, minsleep: {minsleep}, maxscroll: {maxscroll}, minscroll: {minscroll}')

file_path = fr'C:\Users\HP\Desktop\12-17\{query}.json'
num_of_results = get_google_scholar_urls(query, y_start, y_end, num_pages,
                                         minscroll=minscroll, maxscroll=maxscroll,
                                         minsleep=minsleep, maxsleep=maxsleep)

print('')
print('----------------------------------------')
time_passed = time.time() - start_time
print(f"Saved {num_of_results} Google Scholar URLs.")
print(f'\033[1m{"completed in: ":<15}\033[0m {time_passed:>15.2f} seconds')
print(f'\033[1m{"average: ":<15}\033[0m {time_passed/num_pages:>15.2f} seconds / page')
print(f'{datetime.now().strftime("%H:%M:%S"):>15}')
