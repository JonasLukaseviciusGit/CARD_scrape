import scrapy
import json

class DailyStrengthSpider(scrapy.Spider):
    name = "dailystrength"
    start_urls = ["https://www.dailystrength.org/search?query=pain%20management"]

    custom_settings = {
        "DOWNLOAD_DELAY": 5,  # Add a delay of 5 seconds between requests
        "USER_AGENT": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    }

    def parse(self, response):
        for _ in range(5):
            yield response.follow(response.url, self.click_show_more)

    def click_show_more(self, response):
        yield scrapy.FormRequest.from_response(
            response,
            formid="show-more-form",
            clickdata={"type": "submit"},
            callback=self.parse_content,
        )

    def parse_content(self, response):
        links = []

        for element in response.css("li.newsfeed__item"):
            link = element.css("h2.newsfeed__title a::attr(href)").get()
            if link:
                full_link = f"https://www.dailystrength.org{link}"
                links.append(full_link)

        # Save the list of links to a JSON file
        with open("posts.json", "w") as outfile:
            json.dump(links, outfile)

        self.log(f"Scraped {len(links)} links")
