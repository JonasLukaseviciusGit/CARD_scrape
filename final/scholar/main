from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from fake_useragent import UserAgent
import json
import time
from datetime import datetime
import random


def get_random_user_agent():
    ua = UserAgent()
    return ua.random


def get_google_scholar_urls(query, file_path, num_pages=1,
                            minscroll=2, maxscroll=5,
                            minsleep=5, maxsleep=10):
    query = query.replace(' ', '+')
    print(f'\033[1m{"query:":<20}\033[0m {query}')
    print(f'\033[1m{"setting things up:":<20}\033[0m {datetime.now().strftime("%H:%M:%S"):>15}')
    urls = []

    chrome_options = Options()
    chrome_options.add_argument('--headless')  # Run Chrome in headless mode (without GUI)

    random_user_agent = get_random_user_agent()
    print(f'\033[1m{"random user agent:":<20}\033[0m {random_user_agent}')
    print(f'\033[1m{". . ."}')
    chrome_options.add_argument(f'user-agent={random_user_agent}')  # Set a random user-agent

    chrome_path = r"C:\Users\HP\Desktop\chromedriver\chromedriver-win64\chromedriver.exe"

    service = ChromeService(chrome_path)
    driver = webdriver.Chrome(service=service, options=chrome_options)

    print(f'\033[1m{"starting scraping:":<20}\033[0m {datetime.now().strftime("%H:%M:%S"):>15}')
    count = 0
    for page in range(num_pages):
        urls = []  # Reset the list for each page
        titles = []
        authors = []
        pdfs = []

        url = f"https://scholar.google.com/scholar?start={count}&q={query}&hl=lt&as_sdt=2007"
        print(f'\033[1m\033[92m{"getting page   "}\033[0m {count + 1}{":"}{datetime.now().strftime("%H:%M:%S"):>18}')
        print(f'  {url}')
        driver.get(url)

        time.sleep(random.uniform(minscroll, maxscroll))
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(random.uniform(minscroll, maxscroll))
        print('\033[1m\033[91m' + '-' * 100 + '\033[0m')

        # ---------------------------------------- #
        url_elements = driver.find_elements(By.XPATH, "//h3[@class='gs_rt']//a")
        for result in url_elements:
            urls.append(result.get_attribute('href'))

        title_elements = driver.find_elements(By.XPATH, "//h3[@class='gs_rt']//a")
        for result in title_elements:
            titles.append(result.text)

        author_elements = driver.find_elements(By.XPATH, "//div[@class='gs_ri']//div[@class='gs_a']")
        for author_element in author_elements:
            authors.append(author_element.text.split(' - ')[0].strip())

        #pdf_link_elements = driver.find_elements(By.XPATH, "//div[@class='gs_or_ggsm']//a[contains(@href, '.pdf')]")
        #for pdf_link in pdf_link_elements:
           # print(pdf_link.text)

        pdf_links = [element.get_attribute("href") for element in driver.find_elements(By.XPATH, "//div[@class='gs_or_ggsm']//a")]
        for link in pdf_links:
            print(link)
        # ---------------------------------------- #

        # Save URLs after processing each page
        total = []
        index = 0
        for url in urls:
            total.append({'url': url, 'title': titles[index], 'author': authors[index]})
            index += 1

        save_urls_to_json(total, page + 1, file_path)  # Page numbers start from 1

        sleeptime = random.uniform(minsleep, maxsleep)
        print(f'   sleeping for {sleeptime:.2f} seconds')
        time.sleep(sleeptime)
        count += 10

    driver.quit()
    return urls


def save_urls_to_json(urls, page_number, file_path):
    with open(file_path, 'a') as json_file:
        json.dump({'page': page_number, 'urls': urls}, json_file)
        json_file.write('\n')



# Execute
start_time = time.time()

query = input('Enter query: ')
num_pages = int(input('Enter number of pages: '))
maxsleep = int(input('Enter max sleep: '))

minsleep = int(maxsleep/2)
maxscroll = int(minsleep)
minscroll = int(maxscroll/2)

print(f'maxsleep: {maxsleep}, minsleep: {minsleep}, maxscroll: {maxscroll}, minscroll: {minscroll}')

file_path = fr'C:\Users\HP\Desktop\12-17\{query}.json'
result_urls = get_google_scholar_urls(query, file_path, num_pages,
                                      minscroll=minscroll, maxscroll=maxscroll,
                                      minsleep=minsleep, maxsleep=maxsleep)

print('')
print('----------------------------------------')
num_of_results = len(result_urls)
time_passed = time.time() - start_time
print(f"Saved {num_of_results} Google Scholar URLs.")
print(f'\033[1m{"completed in: ":<15}\033[0m {time_passed:>15.2f} seconds')
print(f'\033[1m{"average: ":<15}\033[0m {time_passed/num_pages:>15.2f} seconds / page')
print(f'{datetime.now().strftime("%H:%M:%S"):>15}')
